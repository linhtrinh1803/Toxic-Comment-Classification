{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read and Explore Data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    \nimport numpy as np\nimport pandas as pd\nimport sklearn\n\n# Libraries and packages for text (pre-)processing \nimport string\nimport re\nimport nltk\n\nprint(\"Python version:\", sys.version)\nprint(\"Version info.:\", sys.version_info)\nprint(\"pandas version:\", pd.__version__)\nprint(\"numpy version:\", np.__version__)\nprint(\"skearn version:\", sklearn.__version__)\nprint(\"re version:\", re.__version__)\nprint(\"nltk version:\", nltk.__version__)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:01:39.778600Z","iopub.execute_input":"2023-03-23T00:01:39.779327Z","iopub.status.idle":"2023-03-23T00:01:41.254370Z","shell.execute_reply.started":"2023-03-23T00:01:39.779263Z","shell.execute_reply":"2023-03-23T00:01:41.252759Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n[GCC 9.4.0]\nVersion info.: sys.version_info(major=3, minor=7, micro=12, releaselevel='final', serial=0)\npandas version: 1.3.5\nnumpy version: 1.21.6\nskearn version: 1.0.2\nre version: 2.2.1\nnltk version: 3.2.4\n/kaggle/input/hatecomments/sample_submission.csv/sample_submission.csv\n/kaggle/input/hatecomments/test_labels.csv/test_labels.csv\n/kaggle/input/hatecomments/train.csv/train.csv\n/kaggle/input/hatecomments/test.csv/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"# read the csv file\ntrain_df = pd.read_csv(\"/kaggle/input/hatecomments/train.csv/train.csv\")\ndisplay(train_df.shape, train_df.head())","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:02:31.342202Z","iopub.execute_input":"2023-03-23T00:02:31.342747Z","iopub.status.idle":"2023-03-23T00:02:33.739980Z","shell.execute_reply.started":"2023-03-23T00:02:31.342675Z","shell.execute_reply":"2023-03-23T00:02:33.738330Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"(159571, 8)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Clean Data\n\n\n### 1. Remove Capitalization\n\nBecause of the variety of capitalization used to construct a sentence, capitalization or lower case is the strategy used in text cleaning most frequently. With this method, every word in the text and document will be projected into the same feature area. If mistakes, slang, acronyms, or informal abbreviations were to be replaced, the issue would only arise in rare instances like the USA or the UK.","metadata":{}},{"cell_type":"code","source":"train_df[\"text_clean\"] = train_df[\"comment_text\"].apply(lambda x: x.lower())\ndisplay(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:04:53.513647Z","iopub.execute_input":"2023-03-23T00:04:53.514343Z","iopub.status.idle":"2023-03-23T00:04:53.804516Z","shell.execute_reply.started":"2023-03-23T00:04:53.514304Z","shell.execute_reply":"2023-03-23T00:04:53.803306Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \\\n0             0        0       0       0              0   \n1             0        0       0       0              0   \n2             0        0       0       0              0   \n3             0        0       0       0              0   \n4             0        0       0       0              0   \n\n                                          text_clean  \n0  explanation\\nwhy the edits made under my usern...  \n1  d'aww! he matches this background colour i'm s...  \n2  hey man, i'm really not trying to edit war. it...  \n3  \"\\nmore\\ni can't make any real suggestions on ...  \n4  you, sir, are my hero. any chance you remember...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>explanation\\nwhy the edits made under my usern...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>d'aww! he matches this background colour i'm s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hey man, i'm really not trying to edit war. it...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>you, sir, are my hero. any chance you remember...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2.Expanding Contractions\n\nExample of contraction: We'll -> We will, or we shoudn't've -> we should not have. By using Contractions package to expand contractions in English","metadata":{}},{"cell_type":"code","source":"!pip install contractions","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:02:41.163688Z","iopub.execute_input":"2023-03-23T00:02:41.164122Z","iopub.status.idle":"2023-03-23T00:02:56.726324Z","shell.execute_reply.started":"2023-03-23T00:02:41.164087Z","shell.execute_reply":"2023-03-23T00:02:56.724453Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nCollecting textsearch>=0.0.21\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nCollecting anyascii\n  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyahocorasick\n  Downloading pyahocorasick-2.0.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (101 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import contractions\n\n# Test\ntest_text = \"\"\"\n            Y'all can't expand contractions I'd think. I'd like to know how I'd done that! \n            We're going to the zoo and I don't think I'll be home for dinner.\n            Theyre going to the zoo and she'll be home for dinner.\n            We should've do it in here but we shouldn't've eat it\n            \"\"\"\nprint(\"Test: \", contractions.fix(test_text))\n\ntrain_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: contractions.fix(x))","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:05:01.107627Z","iopub.execute_input":"2023-03-23T00:05:01.108094Z","iopub.status.idle":"2023-03-23T00:05:07.095245Z","shell.execute_reply.started":"2023-03-23T00:05:01.108055Z","shell.execute_reply":"2023-03-23T00:05:07.093690Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Test:  \n            You all cannot expand contractions I would think. I would like to know how I would done that! \n            We are going to the zoo and I do not think I will be home for dinner.\n            They Are going to the zoo and she will be home for dinner.\n            We should have do it in here but we should not have eat it\n            \n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_df[\"comment_text\"][12]) #notice gang ip, wp\nprint(train_df[\"text_clean\"][12])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:05:07.099426Z","iopub.execute_input":"2023-03-23T00:05:07.099893Z","iopub.status.idle":"2023-03-23T00:05:07.111377Z","shell.execute_reply.started":"2023-03-23T00:05:07.099852Z","shell.execute_reply":"2023-03-23T00:05:07.109668Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Hey... what is it..\n@ | talk .\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\n\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...\nhey... what is it..\n@ | talk .\nwhat is it... an exclusive group of some wp talibans...who are good at destroying, self-appointed purist who gang up any one who asks them questions about their anti-social and destructive (non)-contribution at wp?\n\nask sityush to clean up his behavior than issue me nonsensical warnings...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3. Noise Removal\n\nRemoving unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters (American Standard Code for Information Interchange), or other special characters (symbols, emojis, and other grahic characters)","metadata":{}},{"cell_type":"markdown","source":"#### 3.1 Remove URL","metadata":{}},{"cell_type":"code","source":"def remove_URL(text):\n    \"\"\"\n        Remove URLs from a sample string\n    \"\"\"\n    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:05:14.254526Z","iopub.execute_input":"2023-03-23T00:05:14.254980Z","iopub.status.idle":"2023-03-23T00:05:14.261918Z","shell.execute_reply.started":"2023-03-23T00:05:14.254926Z","shell.execute_reply":"2023-03-23T00:05:14.260188Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# remove urls from the text\ntrain_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))\n\n# double check\nprint(train_df[\"comment_text\"][101])\nprint(train_df[\"text_clean\"][101])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:05:15.039859Z","iopub.execute_input":"2023-03-23T00:05:15.041282Z","iopub.status.idle":"2023-03-23T00:05:15.892581Z","shell.execute_reply.started":"2023-03-23T00:05:15.041222Z","shell.execute_reply":"2023-03-23T00:05:15.891063Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Check the following websites:\n\nhttp://www.iranchamber.com/personalities/farabi/farabi.php\nhttp://www.islam.org.br/%C2%A0al_farabi.htm\nhttp://www.superbeyin.com/sohbet/sohbet.htm\ncheck the following websites:\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_df[\"comment_text\"][91]) #notice the url\nprint(train_df[\"text_clean\"][91])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:05:17.376433Z","iopub.execute_input":"2023-03-23T00:05:17.377095Z","iopub.status.idle":"2023-03-23T00:05:17.385597Z","shell.execute_reply.started":"2023-03-23T00:05:17.377023Z","shell.execute_reply":"2023-03-23T00:05:17.383749Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Transliteration of Russian place names\nIn writing about Moscow Metro for the Malayalam Wikipedia, we are finding it difficult to correctly transliterate the Russian place names. For example, do we pronounce Park Kultury as PAARK KALTTARI or PAARK KALCHCHARI (or perhaps something completely different)? Can somebody please help by transliterating the list given in https://ml.wikipedia.org/wiki/സംവാദം:മോസ്കോ_മെട്രോ. (I am not putting the list here as I don't want to clutter up this page.) Thanks\ntransliteration of russian place names\nin writing about moscow metro for the malayalam wikipedia, we are finding it difficult to correctly transliterate the russian place names. for example, do we pronounce park kultury as paark kalttari or paark kalchchari (or perhaps something completely different)? can somebody please help by transliterating the list given in  (i am not putting the list here as i do not want to clutter up this page.) thanks\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3.2 Remove HTML Tags","metadata":{}},{"cell_type":"code","source":"def remove_html(text):\n    \"\"\"\n        Remove the html in sample text\n    \"\"\"\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    return re.sub(html, \"\", text)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:36.985861Z","iopub.execute_input":"2023-03-23T00:06:36.986448Z","iopub.status.idle":"2023-03-23T00:06:36.993820Z","shell.execute_reply.started":"2023-03-23T00:06:36.986405Z","shell.execute_reply":"2023-03-23T00:06:36.992233Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# remove html from the text\ntrain_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))\n\n# double check\nprint(train_df[\"comment_text\"][117]) #notice the url is gone\nprint(train_df[\"text_clean\"][117])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:38.430610Z","iopub.execute_input":"2023-03-23T00:06:38.431147Z","iopub.status.idle":"2023-03-23T00:06:39.368828Z","shell.execute_reply.started":"2023-03-23T00:06:38.431105Z","shell.execute_reply":"2023-03-23T00:06:39.367260Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Also see this if you cant trust Murkoth Ramunni\nhttp://books.google.com/books?id=HHev0U1GfpEC&pg;=PA51&dq;=Thiyya+matrilineal&hl;=en&sa;=X&ei;=TlpPUd2aH8mWiQLgvIDgBA&ved;=0CDYQ6AEwAQ#v=onepage&q;=Thiyya%20matrilineal&f;=false\nalso see this if you cannot trust murkoth ramunni\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3.3 Remove Non-ASCII","metadata":{}},{"cell_type":"code","source":"def remove_non_ascii(text):\n    \"\"\"\n        Remove non-ASCII characters \n    \"\"\"\n    return re.sub(r'[^\\x00-\\x7f]',r'', text) # or ''.join([x for x in text if x in string.printable]) ","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:41.184991Z","iopub.execute_input":"2023-03-23T00:06:41.185504Z","iopub.status.idle":"2023-03-23T00:06:41.192477Z","shell.execute_reply.started":"2023-03-23T00:06:41.185460Z","shell.execute_reply":"2023-03-23T00:06:41.190665Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# remove non-ascii characters from the text\ntrain_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))\n\n# double che\nprint(train_df[\"comment_text\"][6011]) #notice Côte on the 2nd line\nprint(train_df[\"text_clean\"][6011])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:42.304074Z","iopub.execute_input":"2023-03-23T00:06:42.304550Z","iopub.status.idle":"2023-03-23T00:06:43.030685Z","shell.execute_reply.started":"2023-03-23T00:06:42.304509Z","shell.execute_reply":"2023-03-23T00:06:43.029348Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\"\nNo, we don't.  The name \"\"Ivory Coast\"\" contains only English-alphabet letters, and it would sort just fine in almost all cases where \"\"Côte d'Ivoire\"\" remains missorted, often appearing after Croatia, after Cyprus, after Czech Republic.  You can see this by looking at Category:Communications by country and seeing where the subcategory Category:Communications in Côte d'Ivoire is missorted (even though the sorting of the article of the same name has been fixed—can you see the difference in order?).  Or you can see it by looking at Category:Nations at the 2004 Summer Olympics and seeing where the article Côte d'Ivoire at the 2004 Summer Olympics is missorted.\nThere are hundreds of those problems out there.  Why don't you make yourself useful, and go fix some of them?   \n\n\"\n\"\nno, we do not.  the name \"\"ivory coast\"\" contains only english-alphabet letters, and it would sort just fine in almost all cases where \"\"cte d'ivoire\"\" remains missorted, often appearing after croatia, after cyprus, after czech republic.  you can see this by looking at category:communications by country and seeing where the subcategory category:communications in cte d'ivoire is missorted (even though the sorting of the article of the same name has been fixedcan you see the difference in order?).  or you can see it by looking at category:nations at the 2004 summer olympics and seeing where the article cte d'ivoire at the 2004 summer olympics is missorted.\nthere are hundreds of those problems out there.  why do not you make yourself useful, and go fix some of them?   \n\n\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_df[\"comment_text\"][7814]) #notice eôs\nprint(train_df[\"text_clean\"][7814])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:45.488382Z","iopub.execute_input":"2023-03-23T00:06:45.489411Z","iopub.status.idle":"2023-03-23T00:06:45.497227Z","shell.execute_reply.started":"2023-03-23T00:06:45.489364Z","shell.execute_reply":"2023-03-23T00:06:45.495335Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"amazons, after all\nI think I found the source of the connection of the labrys with the amazons: sagaris \n\n saga^ris , eôs Ion. ios, hê; pl. sagareis Ion. -i_s:a weapon used by the Scythian tribes, Hdt.1.215, 4.5;\n\n A. axinas sagaris eichon Id.7.64 ; by the Amazons, Aristarch. in \n PAmh.2.12 ii 10; by the Persians, Amazons, Mossynoeci, etc., \n X.An.4.4.16, 5.4.13:acc. to Hsch. single-edged, and joined by \n X. with kopis and machaira, Cyr. 1.2.9, 2.1.9, 4.2.22; \n double-edged acc. to AP6.94 (Phil.).\n\ni.e. an axe-like weapon, sometimes described as single-edged, and sometimes as double-edged. This shouild probably be put in a Sagaris article.\namazons, after all\ni think i found the source of the connection of the labrys with the amazons: sagaris \n\n saga^ris , es ion. ios, h; pl. sagareis ion. -i_s:a weapon used by the scythian tribes, hdt.1.215, 4.5;\n\n a. axinas sagaris eichon id.7.64 ; by the amazons, aristarch. in \n pamh.2.12 ii 10; by the persians, amazons, mossynoeci, etc., \n x.an.4.4.16, 5.4.13:acc. to hsch. single-edged, and joined by \n x. with kopis and machaira, cyr. 1.2.9, 2.1.9, 4.2.22; \n double-edged acc. to ap6.94 (phil.).\n\ni.e. an axe-like weapon, sometimes described as single-edged, and sometimes as double-edged. this shouild probably be put in a sagaris article.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3.4 Remove Special Characters\n\nSpecial characters could be symbols, emojis, and other graphic characters. ","metadata":{}},{"cell_type":"code","source":"def remove_special_characters(text):\n    \"\"\"\n        Remove special special characters, including symbols, emojis, and other graphic characters\n    \"\"\"\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:48.306744Z","iopub.execute_input":"2023-03-23T00:06:48.307797Z","iopub.status.idle":"2023-03-23T00:06:48.315762Z","shell.execute_reply.started":"2023-03-23T00:06:48.307726Z","shell.execute_reply":"2023-03-23T00:06:48.314239Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_special_characters(x))","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:49.441446Z","iopub.execute_input":"2023-03-23T00:06:49.441999Z","iopub.status.idle":"2023-03-23T00:06:51.405525Z","shell.execute_reply.started":"2023-03-23T00:06:49.441939Z","shell.execute_reply":"2023-03-23T00:06:51.404235Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# double check\nprint(train_df[\"comment_text\"][143]) #notice the telephone\nprint(train_df[\"text_clean\"][143])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:51.407588Z","iopub.execute_input":"2023-03-23T00:06:51.408352Z","iopub.status.idle":"2023-03-23T00:06:51.417030Z","shell.execute_reply.started":"2023-03-23T00:06:51.408307Z","shell.execute_reply":"2023-03-23T00:06:51.415454Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\"P.S. It's not polite to talk to people behind their backs, please remove your comments from Mrph's talk page.\n\nVaughan\nYou're right; I went to check your previous edit and found a page on the Marvel site that spelled it \"\"Vaughn\"\", but now I am finding many more that spell it correctly. Thanks for the edits.   (☎☓) \n\n\"\n\"p.s. it is not polite to talk to people behind their backs, please remove your comments from mrph's talk page.\n\nvaughan\nyou are right; i went to check your previous edit and found a page on the marvel site that spelled it \"\"vaughn\"\", but now i am finding many more that spell it correctly. thanks for the edits.   () \n\n\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_df[\"comment_text\"][189]) #notice the heart\nprint(train_df[\"text_clean\"][189])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:51.947445Z","iopub.execute_input":"2023-03-23T00:06:51.948621Z","iopub.status.idle":"2023-03-23T00:06:51.956166Z","shell.execute_reply.started":"2023-03-23T00:06:51.948569Z","shell.execute_reply":"2023-03-23T00:06:51.954469Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\"\n\nSorry to interrupt but I'm at 1200 edits now... the first 200 were likely just on my own pages and because I was asking for help so much so maybe just 1000... or maybe less... but it still kind of counts. ♥♥Amulet♥♥ \"\n\"\n\nsorry to interrupt but i am at 1200 edits now... the first 200 were likely just on my own pages and because i was asking for help so much so maybe just 1000... or maybe less... but it still kind of counts. amulet \"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3.5 Remove Punctuations","metadata":{}},{"cell_type":"code","source":"def remove_punct(text):\n    \"\"\"\n        Remove the punctuation\n    \"\"\"\n#     return re.sub(r'[]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~-]+', \"\", text)\n    return text.translate(str.maketrans('', '', string.punctuation))","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:55.989536Z","iopub.execute_input":"2023-03-23T00:06:55.990792Z","iopub.status.idle":"2023-03-23T00:06:55.997833Z","shell.execute_reply.started":"2023-03-23T00:06:55.990741Z","shell.execute_reply":"2023-03-23T00:06:55.996242Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_punct(x))\n\n# double check\nprint(train_df[\"comment_text\"][3])\nprint(train_df[\"text_clean\"][3])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:06:56.567329Z","iopub.execute_input":"2023-03-23T00:06:56.568222Z","iopub.status.idle":"2023-03-23T00:06:57.633692Z","shell.execute_reply.started":"2023-03-23T00:06:56.568176Z","shell.execute_reply":"2023-03-23T00:06:57.632263Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"\"\nMore\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\n\nmore\ni cannot make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know\n\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it is listed in the relevant form eg wikipediagoodarticlenominationstransport  \n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_df[\"comment_text\"][7595])\nprint(train_df[\"text_clean\"][7595])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:07:02.833300Z","iopub.execute_input":"2023-03-23T00:07:02.833817Z","iopub.status.idle":"2023-03-23T00:07:02.840840Z","shell.execute_reply.started":"2023-03-23T00:07:02.833777Z","shell.execute_reply":"2023-03-23T00:07:02.839498Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\"\n\nI don't see a problem with citing GAO/HRD-87-46 and the pdf linked there. The \"\"internal FDA documents\"\" sound like they are by definition not published.  Talk \"\n\n\ni do not see a problem with citing gaohrd8746 and the pdf linked there the internal fda documents sound like they are by definition not published  talk \n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3.6 Other Manual Text Cleaning Tasks\n\n- Replace Unicode characters with equivalent ASCII character (instead of removing)\n- Replace the entity references with their actual symbols instead of removing as HTML tags\n- Replace the Typos, slang, acronyms or informal abbreviations - depends on diffrent situations or main topics of the NLP such as finance or medical topics. \n- List out all the hashtags/ usernames then replace with equivalent words. \n- Replace the emoticon/ emoji with equivalent word meaning such as \":)\" with \"smile\" \n- Spelling correction","metadata":{}},{"cell_type":"code","source":"def other_clean(text):\n        \"\"\"\n            Other manual text cleaning techniques\n        \"\"\"\n        # Typos, slang and other\n        sample_typos_slang = {\n                                \"w/e\": \"whatever\",\n                                \"usagov\": \"usa government\",\n                                \"recentlu\": \"recently\",\n                                \"ph0tos\": \"photos\",\n                                \"amirite\": \"am i right\",\n                                \"exp0sed\": \"exposed\",\n                                \"<3\": \"love\",\n                                \"luv\": \"love\",\n                                \"amageddon\": \"armageddon\",\n                                \"trfc\": \"traffic\",\n                                \"16yr\": \"16 year\"\n                                }\n\n        # Acronyms\n        sample_acronyms =  { \n                            \"mh370\": \"malaysia airlines flight 370\",\n                            \"okwx\": \"oklahoma city weather\",\n                            \"arwx\": \"arkansas weather\",    \n                            \"gawx\": \"georgia weather\",  \n                            \"scwx\": \"south carolina weather\",  \n                            \"cawx\": \"california weather\",\n                            \"tnwx\": \"tennessee weather\",\n                            \"azwx\": \"arizona weather\",  \n                            \"alwx\": \"alabama weather\",\n                            \"usnwsgov\": \"united states national weather service\",\n                            \"2mw\": \"tomorrow\"\n                            }\n\n        # Some common abbreviations \n        sample_abbr = {\n                        \"$\" : \" dollar \",\n                        \"€\" : \" euro \",\n                        \"4ao\" : \"for adults only\",\n                        \"a.m\" : \"before midday\",\n                        \"a3\" : \"anytime anywhere anyplace\",\n                        \"aamof\" : \"as a matter of fact\",\n                        \"acct\" : \"account\",\n                        \"adih\" : \"another day in hell\",\n                        \"afaic\" : \"as far as i am concerned\",\n                        \"afaict\" : \"as far as i can tell\",\n                        \"afaik\" : \"as far as i know\",\n                        \"afair\" : \"as far as i remember\",\n                        \"afk\" : \"away from keyboard\",\n                        \"app\" : \"application\",\n                        \"approx\" : \"approximately\",\n                        \"apps\" : \"applications\",\n                        \"asap\" : \"as soon as possible\",\n                        \"asl\" : \"age, sex, location\",\n                        \"atk\" : \"at the keyboard\",\n                        \"ave.\" : \"avenue\",\n                        \"aymm\" : \"are you my mother\",\n                        \"ayor\" : \"at your own risk\", \n                        \"b&b\" : \"bed and breakfast\",\n                        \"b+b\" : \"bed and breakfast\",\n                        \"b.c\" : \"before christ\",\n                        \"b2b\" : \"business to business\",\n                        \"b2c\" : \"business to customer\",\n                        \"b4\" : \"before\",\n                        \"b4n\" : \"bye for now\",\n                        \"b@u\" : \"back at you\",\n                        \"bae\" : \"before anyone else\",\n                        \"bak\" : \"back at keyboard\",\n                        \"bbbg\" : \"bye bye be good\",\n                        \"bbc\" : \"british broadcasting corporation\",\n                        \"bbias\" : \"be back in a second\",\n                        \"bbl\" : \"be back later\",\n                        \"bbs\" : \"be back soon\",\n                        \"be4\" : \"before\",\n                        \"bfn\" : \"bye for now\",\n                        \"blvd\" : \"boulevard\",\n                        \"bout\" : \"about\",\n                        \"brb\" : \"be right back\",\n                        \"bros\" : \"brothers\",\n                        \"brt\" : \"be right there\",\n                        \"bsaaw\" : \"big smile and a wink\",\n                        \"btw\" : \"by the way\",\n                        \"bwl\" : \"bursting with laughter\",\n                        \"c/o\" : \"care of\",\n                        \"cet\" : \"central european time\",\n                        \"cf\" : \"compare\",\n                        \"cia\" : \"central intelligence agency\",\n                        \"csl\" : \"can not stop laughing\",\n                        \"cu\" : \"see you\",\n                        \"cul8r\" : \"see you later\",\n                        \"cv\" : \"curriculum vitae\",\n                        \"cwot\" : \"complete waste of time\",\n                        \"cya\" : \"see you\",\n                        \"cyt\" : \"see you tomorrow\",\n                        \"dae\" : \"does anyone else\",\n                        \"dbmib\" : \"do not bother me i am busy\",\n                        \"diy\" : \"do it yourself\",\n                        \"dm\" : \"direct message\",\n                        \"dwh\" : \"during work hours\",\n                        \"e123\" : \"easy as one two three\",\n                        \"eet\" : \"eastern european time\",\n                        \"eg\" : \"example\",\n                        \"embm\" : \"early morning business meeting\",\n                        \"encl\" : \"enclosed\",\n                        \"encl.\" : \"enclosed\",\n                        \"etc\" : \"and so on\",\n                        \"faq\" : \"frequently asked questions\",\n                        \"fawc\" : \"for anyone who cares\",\n                        \"fb\" : \"facebook\",\n                        \"fc\" : \"fingers crossed\",\n                        \"fig\" : \"figure\",\n                        \"fimh\" : \"forever in my heart\", \n                        \"ft.\" : \"feet\",\n                        \"ft\" : \"featuring\",\n                        \"ftl\" : \"for the loss\",\n                        \"ftw\" : \"for the win\",\n                        \"fwiw\" : \"for what it is worth\",\n                        \"fyi\" : \"for your information\",\n                        \"g9\" : \"genius\",\n                        \"gahoy\" : \"get a hold of yourself\",\n                        \"gal\" : \"get a life\",\n                        \"gcse\" : \"general certificate of secondary education\",\n                        \"gfn\" : \"gone for now\",\n                        \"gg\" : \"good game\",\n                        \"gl\" : \"good luck\",\n                        \"glhf\" : \"good luck have fun\",\n                        \"gmt\" : \"greenwich mean time\",\n                        \"gmta\" : \"great minds think alike\",\n                        \"gn\" : \"good night\",\n                        \"g.o.a.t\" : \"greatest of all time\",\n                        \"goat\" : \"greatest of all time\",\n                        \"goi\" : \"get over it\",\n                        \"gps\" : \"global positioning system\",\n                        \"gr8\" : \"great\",\n                        \"gratz\" : \"congratulations\",\n                        \"gyal\" : \"girl\",\n                        \"h&c\" : \"hot and cold\",\n                        \"hp\" : \"horsepower\",\n                        \"hr\" : \"hour\",\n                        \"hrh\" : \"his royal highness\",\n                        \"ht\" : \"height\",\n                        \"ibrb\" : \"i will be right back\",\n                        \"ic\" : \"i see\",\n                        \"icq\" : \"i seek you\",\n                        \"icymi\" : \"in case you missed it\",\n                        \"idc\" : \"i do not care\",\n                        \"idgadf\" : \"i do not give a damn fuck\",\n                        \"idgaf\" : \"i do not give a fuck\",\n                        \"idk\" : \"i do not know\",\n                        \"ie\" : \"that is\",\n                        \"i.e\" : \"that is\",\n                        \"ifyp\" : \"i feel your pain\",\n                        \"IG\" : \"instagram\",\n                        \"iirc\" : \"if i remember correctly\",\n                        \"ilu\" : \"i love you\",\n                        \"ily\" : \"i love you\",\n                        \"imho\" : \"in my humble opinion\",\n                        \"imo\" : \"in my opinion\",\n                        \"imu\" : \"i miss you\",\n                        \"iow\" : \"in other words\",\n                        \"irl\" : \"in real life\",\n                        \"j4f\" : \"just for fun\",\n                        \"jic\" : \"just in case\",\n                        \"jk\" : \"just kidding\",\n                        \"jsyk\" : \"just so you know\",\n                        \"l8r\" : \"later\",\n                        \"lb\" : \"pound\",\n                        \"lbs\" : \"pounds\",\n                        \"ldr\" : \"long distance relationship\",\n                        \"lmao\" : \"laugh my ass off\",\n                        \"lmfao\" : \"laugh my fucking ass off\",\n                        \"lol\" : \"laughing out loud\",\n                        \"ltd\" : \"limited\",\n                        \"ltns\" : \"long time no see\",\n                        \"m8\" : \"mate\",\n                        \"mf\" : \"motherfucker\",\n                        \"mfs\" : \"motherfuckers\",\n                        \"mfw\" : \"my face when\",\n                        \"mofo\" : \"motherfucker\",\n                        \"mph\" : \"miles per hour\",\n                        \"mr\" : \"mister\",\n                        \"mrw\" : \"my reaction when\",\n                        \"ms\" : \"miss\",\n                        \"mte\" : \"my thoughts exactly\",\n                        \"nagi\" : \"not a good idea\",\n                        \"nbc\" : \"national broadcasting company\",\n                        \"nbd\" : \"not big deal\",\n                        \"nfs\" : \"not for sale\",\n                        \"ngl\" : \"not going to lie\",\n                        \"nhs\" : \"national health service\",\n                        \"nrn\" : \"no reply necessary\",\n                        \"nsfl\" : \"not safe for life\",\n                        \"nsfw\" : \"not safe for work\",\n                        \"nth\" : \"nice to have\",\n                        \"nvr\" : \"never\",\n                        \"nyc\" : \"new york city\",\n                        \"oc\" : \"original content\",\n                        \"og\" : \"original\",\n                        \"ohp\" : \"overhead projector\",\n                        \"oic\" : \"oh i see\",\n                        \"omdb\" : \"over my dead body\",\n                        \"omg\" : \"oh my god\",\n                        \"omw\" : \"on my way\",\n                        \"p.a\" : \"per annum\",\n                        \"p.m\" : \"after midday\",\n                        \"pm\" : \"prime minister\",\n                        \"poc\" : \"people of color\",\n                        \"pov\" : \"point of view\",\n                        \"pp\" : \"pages\",\n                        \"ppl\" : \"people\",\n                        \"prw\" : \"parents are watching\",\n                        \"ps\" : \"postscript\",\n                        \"pt\" : \"point\",\n                        \"ptb\" : \"please text back\",\n                        \"pto\" : \"please turn over\",\n                        \"qpsa\" : \"what happens\", #\"que pasa\",\n                        \"ratchet\" : \"rude\",\n                        \"rbtl\" : \"read between the lines\",\n                        \"rlrt\" : \"real life retweet\", \n                        \"rofl\" : \"rolling on the floor laughing\",\n                        \"roflol\" : \"rolling on the floor laughing out loud\",\n                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n                        \"rt\" : \"retweet\",\n                        \"ruok\" : \"are you ok\",\n                        \"sfw\" : \"safe for work\",\n                        \"sk8\" : \"skate\",\n                        \"smh\" : \"shake my head\",\n                        \"sq\" : \"square\",\n                        \"srsly\" : \"seriously\", \n                        \"ssdd\" : \"same stuff different day\",\n                        \"tbh\" : \"to be honest\",\n                        \"tbs\" : \"tablespooful\",\n                        \"tbsp\" : \"tablespooful\",\n                        \"tfw\" : \"that feeling when\",\n                        \"thks\" : \"thank you\",\n                        \"tho\" : \"though\",\n                        \"thx\" : \"thank you\",\n                        \"tia\" : \"thanks in advance\",\n                        \"til\" : \"today i learned\",\n                        \"tl;dr\" : \"too long i did not read\",\n                        \"tldr\" : \"too long i did not read\",\n                        \"tmb\" : \"tweet me back\",\n                        \"tntl\" : \"trying not to laugh\",\n                        \"ttyl\" : \"talk to you later\",\n                        \"u\" : \"you\",\n                        \"u2\" : \"you too\",\n                        \"u4e\" : \"yours for ever\",\n                        \"utc\" : \"coordinated universal time\",\n                        \"w/\" : \"with\",\n                        \"w/o\" : \"without\",\n                        \"w8\" : \"wait\",\n                        \"wassup\" : \"what is up\",\n                        \"wb\" : \"welcome back\",\n                        \"wtf\" : \"what the fuck\",\n                        \"wtg\" : \"way to go\",\n                        \"wtpa\" : \"where the party at\",\n                        \"wuf\" : \"where are you from\",\n                        \"wuzup\" : \"what is up\",\n                        \"wywh\" : \"wish you were here\",\n                        \"yd\" : \"yard\",\n                        \"ygtr\" : \"you got that right\",\n                        \"ynk\" : \"you never know\",\n                        \"zzz\" : \"sleeping bored and tired\"\n                        }\n            \n        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n        \n        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n        \n        return text","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:07:04.668120Z","iopub.execute_input":"2023-03-23T00:07:04.669589Z","iopub.status.idle":"2023-03-23T00:07:04.713585Z","shell.execute_reply.started":"2023-03-23T00:07:04.669509Z","shell.execute_reply":"2023-03-23T00:07:04.711938Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: other_clean(x))","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:07:05.881814Z","iopub.execute_input":"2023-03-23T00:07:05.882661Z","iopub.status.idle":"2023-03-23T00:07:57.254492Z","shell.execute_reply.started":"2023-03-23T00:07:05.882620Z","shell.execute_reply":"2023-03-23T00:07:57.253069Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# double check\nprint(train_df[\"comment_text\"][10875]) #notice omg\nprint(train_df[\"text_clean\"][10875])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:07:57.259688Z","iopub.execute_input":"2023-03-23T00:07:57.260446Z","iopub.status.idle":"2023-03-23T00:07:57.269371Z","shell.execute_reply.started":"2023-03-23T00:07:57.260402Z","shell.execute_reply":"2023-03-23T00:07:57.267762Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"\"   Please refrain from creating inappropriate pages such as OMG LIEK TOTALLY. It is considered vandalism. If you would like to experiment, use the sandbox.  , Recent changes patrol\"\n   please refrain from creating inappropriate pages such as oh my god liek totally it is considered vandalism if you would like to experiment use the sandbox   recent changes patrol\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_df[\"comment_text\"][20296]) #notice lmao\nprint(train_df[\"text_clean\"][20296])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:07:57.271425Z","iopub.execute_input":"2023-03-23T00:07:57.271913Z","iopub.status.idle":"2023-03-23T00:07:57.286858Z","shell.execute_reply.started":"2023-03-23T00:07:57.271860Z","shell.execute_reply":"2023-03-23T00:07:57.285828Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Poor Walty is blocked ( LMAO knew it would happen someday.\npoor walty is blocked  laugh my ass off knew it would happen someday\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3.7 Spelling Correction (optional)\n\nSocial media text data are usually typos or mistyped. ","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nprint(\"Test: \", TextBlob(\"sleapy and tehre is no plaxe I'm gioong to.\").correct())","metadata":{"execution":{"iopub.status.busy":"2023-03-19T05:57:47.601147Z","iopub.execute_input":"2023-03-19T05:57:47.601521Z","iopub.status.idle":"2023-03-19T05:57:47.778777Z","shell.execute_reply.started":"2023-03-19T05:57:47.601488Z","shell.execute_reply":"2023-03-19T05:57:47.777113Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"Test:  sleepy and there is no place I'm going to.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df['text_clean'] = train_df['text_clean'].apply(lambda x: TextBlob(x).correct())","metadata":{"execution":{"iopub.status.busy":"2023-03-19T05:57:50.608818Z","iopub.execute_input":"2023-03-19T05:57:50.609184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# double check\nprint(train_df[\"comment_text\"][10875]) #notice liek is fixed to like \nprint(train_df[\"text_clean\"][10875])","metadata":{"execution":{"iopub.status.busy":"2023-03-19T05:37:53.439930Z","iopub.execute_input":"2023-03-19T05:37:53.440531Z","iopub.status.idle":"2023-03-19T05:37:53.445959Z","shell.execute_reply.started":"2023-03-19T05:37:53.440492Z","shell.execute_reply":"2023-03-19T05:37:53.445031Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"\"   Please refrain from creating inappropriate pages such as OMG LIEK TOTALLY. It is considered vandalism. If you would like to experiment, use the sandbox.  , Recent changes patrol\"\n   Please refrain from creating inappropriate pages such as OMG LIEK TOTALLY It is considered vandalism If you would like to experiment use the sandbox   Recent changes patrol\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Preprocessing:\n\n### 1. Tokenization\nTokenization is a common technique that split a sentence into tokens, where a token could be characters, words, phrases, symbols, or other meaningful elements. By breaking sentences into smaller chunks, that would help to investigate the words in a sentence and also the subsequent steps in the NLP pipeline, such as stemming.","metadata":{}},{"cell_type":"code","source":"# Tokenizing the tweet base texts.\nfrom nltk.tokenize import word_tokenize\n\ntrain_df['tokenized'] = train_df['text_clean'].apply(word_tokenize)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:10:39.245218Z","iopub.execute_input":"2023-03-23T00:10:39.246895Z","iopub.status.idle":"2023-03-23T00:11:34.964999Z","shell.execute_reply.started":"2023-03-23T00:10:39.246829Z","shell.execute_reply":"2023-03-23T00:11:34.963562Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \\\n0             0        0       0       0              0   \n1             0        0       0       0              0   \n2             0        0       0       0              0   \n3             0        0       0       0              0   \n4             0        0       0       0              0   \n\n                                          text_clean  \\\n0  explanation\\nwhy the edits made under my usern...   \n1  daww he matches this background colour i am se...   \n2  hey man i am really not trying to edit war it ...   \n3  \\nmore\\ni cannot make any real suggestions on ...   \n4  you sir are my hero any chance you remember wh...   \n\n                                           tokenized  \n0  [explanation, why, the, edits, made, under, my...  \n1  [daww, he, matches, this, background, colour, ...  \n2  [hey, man, i, am, really, not, trying, to, edi...  \n3  [more, i, can, not, make, any, real, suggestio...  \n4  [you, sir, are, my, hero, any, chance, you, re...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>text_clean</th>\n      <th>tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>explanation\\nwhy the edits made under my usern...</td>\n      <td>[explanation, why, the, edits, made, under, my...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>daww he matches this background colour i am se...</td>\n      <td>[daww, he, matches, this, background, colour, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hey man i am really not trying to edit war it ...</td>\n      <td>[hey, man, i, am, really, not, trying, to, edi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>\\nmore\\ni cannot make any real suggestions on ...</td>\n      <td>[more, i, can, not, make, any, real, suggestio...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>you sir are my hero any chance you remember wh...</td>\n      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2. Remove Stop Words (or/and Frequent words/ Rare words):\nStop words are common words in any language that occur with a high frequency but do not deliver meaningful information for the whole sentence. For example, {“a”, “about”, “above”, “across”, “after”, “afterward”, “again”, ...} can be considered as stop words","metadata":{}},{"cell_type":"code","source":"# Removing stopwords.\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words('english'))\ntrain_df['stopwords_removed'] = train_df['tokenized'].apply(lambda x: [word for word in x if word not in stop])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:12:48.388994Z","iopub.execute_input":"2023-03-23T00:12:48.389545Z","iopub.status.idle":"2023-03-23T00:12:51.079800Z","shell.execute_reply.started":"2023-03-23T00:12:48.389499Z","shell.execute_reply":"2023-03-23T00:12:51.078467Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \\\n0             0        0       0       0              0   \n1             0        0       0       0              0   \n2             0        0       0       0              0   \n3             0        0       0       0              0   \n4             0        0       0       0              0   \n\n                                          text_clean  \\\n0  explanation\\nwhy the edits made under my usern...   \n1  daww he matches this background colour i am se...   \n2  hey man i am really not trying to edit war it ...   \n3  \\nmore\\ni cannot make any real suggestions on ...   \n4  you sir are my hero any chance you remember wh...   \n\n                                           tokenized  \\\n0  [explanation, why, the, edits, made, under, my...   \n1  [daww, he, matches, this, background, colour, ...   \n2  [hey, man, i, am, really, not, trying, to, edi...   \n3  [more, i, can, not, make, any, real, suggestio...   \n4  [you, sir, are, my, hero, any, chance, you, re...   \n\n                                   stopwords_removed  \n0  [explanation, edits, made, username, hardcore,...  \n1  [daww, matches, background, colour, seemingly,...  \n2  [hey, man, really, trying, edit, war, guy, con...  \n3  [make, real, suggestions, improvement, wondere...  \n4                [sir, hero, chance, remember, page]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>text_clean</th>\n      <th>tokenized</th>\n      <th>stopwords_removed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>explanation\\nwhy the edits made under my usern...</td>\n      <td>[explanation, why, the, edits, made, under, my...</td>\n      <td>[explanation, edits, made, username, hardcore,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>daww he matches this background colour i am se...</td>\n      <td>[daww, he, matches, this, background, colour, ...</td>\n      <td>[daww, matches, background, colour, seemingly,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hey man i am really not trying to edit war it ...</td>\n      <td>[hey, man, i, am, really, not, trying, to, edi...</td>\n      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>\\nmore\\ni cannot make any real suggestions on ...</td>\n      <td>[more, i, can, not, make, any, real, suggestio...</td>\n      <td>[make, real, suggestions, improvement, wondere...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>you sir are my hero any chance you remember wh...</td>\n      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n      <td>[sir, hero, chance, remember, page]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3. Stemming\nStemming is a process of extracting a root word - identifying a common stem among various forms (e.g., singular and plural noun form) of a word, for example, the words \"gardening\", \"gardener\" or \"gardens\" share the same stem, garden. Stemming uproots suffixes from words to merge words with similar meanings under their standard stem.\n\nThere are three major stemming algorithms in use nowadays:\n\n- Porter - PorterStemmer()): This stemming algorithm is an older one. It’s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form. It’s not too complex and development on it is frozen. Typically, it’s a nice starting basic stemmer, but it’s not really advised to use it for any production/complex application. Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. It also is a very gentle stemming algorithm when compared to others.\n\n- Snowball - LancasterStemmer(): This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5% difference in the way that Snowball stems versus Porter.\n\n- Lancaster - SnowballStemmer(): Just for fun, the Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily. It’s a good choice for that. One complaint around this stemming algorithm though is that it sometimes is overly aggressive and can really transform words into strange stems. Just make sure it does what you want it to before you go with this option!","metadata":{}},{"cell_type":"markdown","source":"#### Snowball","metadata":{}},{"cell_type":"code","source":"from nltk.stem import SnowballStemmer\n\ndef snowball_stemmer(text):\n    \"\"\"\n        Stem words in list of tokenized words with SnowballStemmer\n    \"\"\"\n    stemmer = nltk.SnowballStemmer(\"english\")\n    stems = [stemmer.stem(i) for i in text]\n    return stems","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:17:51.197605Z","iopub.execute_input":"2023-03-23T00:17:51.198949Z","iopub.status.idle":"2023-03-23T00:17:51.206217Z","shell.execute_reply.started":"2023-03-23T00:17:51.198896Z","shell.execute_reply":"2023-03-23T00:17:51.204510Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_df['snowball_stemmer'] = train_df['stopwords_removed'].apply(lambda x: snowball_stemmer(x))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:18:02.212747Z","iopub.execute_input":"2023-03-23T00:18:02.213634Z","iopub.status.idle":"2023-03-23T00:19:30.633190Z","shell.execute_reply.started":"2023-03-23T00:18:02.213584Z","shell.execute_reply":"2023-03-23T00:19:30.632013Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \\\n0             0        0       0       0              0   \n1             0        0       0       0              0   \n2             0        0       0       0              0   \n3             0        0       0       0              0   \n4             0        0       0       0              0   \n\n                                          text_clean  \\\n0  explanation\\nwhy the edits made under my usern...   \n1  daww he matches this background colour i am se...   \n2  hey man i am really not trying to edit war it ...   \n3  \\nmore\\ni cannot make any real suggestions on ...   \n4  you sir are my hero any chance you remember wh...   \n\n                                           tokenized  \\\n0  [explanation, why, the, edits, made, under, my...   \n1  [daww, he, matches, this, background, colour, ...   \n2  [hey, man, i, am, really, not, trying, to, edi...   \n3  [more, i, can, not, make, any, real, suggestio...   \n4  [you, sir, are, my, hero, any, chance, you, re...   \n\n                                   stopwords_removed  \\\n0  [explanation, edits, made, username, hardcore,...   \n1  [daww, matches, background, colour, seemingly,...   \n2  [hey, man, really, trying, edit, war, guy, con...   \n3  [make, real, suggestions, improvement, wondere...   \n4                [sir, hero, chance, remember, page]   \n\n                                    snowball_stemmer  \n0  [explan, edit, made, usernam, hardcor, metalli...  \n1  [daww, match, background, colour, seem, stuck,...  \n2  [hey, man, realli, tri, edit, war, guy, consta...  \n3  [make, real, suggest, improv, wonder, section,...  \n4                   [sir, hero, chanc, rememb, page]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>text_clean</th>\n      <th>tokenized</th>\n      <th>stopwords_removed</th>\n      <th>snowball_stemmer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>explanation\\nwhy the edits made under my usern...</td>\n      <td>[explanation, why, the, edits, made, under, my...</td>\n      <td>[explanation, edits, made, username, hardcore,...</td>\n      <td>[explan, edit, made, usernam, hardcor, metalli...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>daww he matches this background colour i am se...</td>\n      <td>[daww, he, matches, this, background, colour, ...</td>\n      <td>[daww, matches, background, colour, seemingly,...</td>\n      <td>[daww, match, background, colour, seem, stuck,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hey man i am really not trying to edit war it ...</td>\n      <td>[hey, man, i, am, really, not, trying, to, edi...</td>\n      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n      <td>[hey, man, realli, tri, edit, war, guy, consta...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>\\nmore\\ni cannot make any real suggestions on ...</td>\n      <td>[more, i, can, not, make, any, real, suggestio...</td>\n      <td>[make, real, suggestions, improvement, wondere...</td>\n      <td>[make, real, suggest, improv, wonder, section,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>you sir are my hero any chance you remember wh...</td>\n      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n      <td>[sir, hero, chance, remember, page]</td>\n      <td>[sir, hero, chanc, rememb, page]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Lancaster","metadata":{}},{"cell_type":"code","source":"from nltk.stem import LancasterStemmer\n\ndef lancaster_stemmer(text):\n    \"\"\"\n        Stem words in list of tokenized words with LancasterStemmer\n    \"\"\"\n    stemmer = nltk.LancasterStemmer()\n    stems = [stemmer.stem(i) for i in text]\n    return stems","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:19:42.823463Z","iopub.execute_input":"2023-03-23T00:19:42.823944Z","iopub.status.idle":"2023-03-23T00:19:42.831744Z","shell.execute_reply.started":"2023-03-23T00:19:42.823905Z","shell.execute_reply":"2023-03-23T00:19:42.830046Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train_df['lancaster_stemmer'] = train_df['stopwords_removed'].apply(lambda x: lancaster_stemmer(x))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-23T00:19:43.560678Z","iopub.execute_input":"2023-03-23T00:19:43.561784Z","iopub.status.idle":"2023-03-23T00:21:38.558380Z","shell.execute_reply.started":"2023-03-23T00:19:43.561729Z","shell.execute_reply":"2023-03-23T00:21:38.557085Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \\\n0             0        0       0       0              0   \n1             0        0       0       0              0   \n2             0        0       0       0              0   \n3             0        0       0       0              0   \n4             0        0       0       0              0   \n\n                                          text_clean  \\\n0  explanation\\nwhy the edits made under my usern...   \n1  daww he matches this background colour i am se...   \n2  hey man i am really not trying to edit war it ...   \n3  \\nmore\\ni cannot make any real suggestions on ...   \n4  you sir are my hero any chance you remember wh...   \n\n                                           tokenized  \\\n0  [explanation, why, the, edits, made, under, my...   \n1  [daww, he, matches, this, background, colour, ...   \n2  [hey, man, i, am, really, not, trying, to, edi...   \n3  [more, i, can, not, make, any, real, suggestio...   \n4  [you, sir, are, my, hero, any, chance, you, re...   \n\n                                   stopwords_removed  \\\n0  [explanation, edits, made, username, hardcore,...   \n1  [daww, matches, background, colour, seemingly,...   \n2  [hey, man, really, trying, edit, war, guy, con...   \n3  [make, real, suggestions, improvement, wondere...   \n4                [sir, hero, chance, remember, page]   \n\n                                    snowball_stemmer  \\\n0  [explan, edit, made, usernam, hardcor, metalli...   \n1  [daww, match, background, colour, seem, stuck,...   \n2  [hey, man, realli, tri, edit, war, guy, consta...   \n3  [make, real, suggest, improv, wonder, section,...   \n4                   [sir, hero, chanc, rememb, page]   \n\n                                   lancaster_stemmer  \n0  [expl, edit, mad, usernam, hardc, metallic, fa...  \n1  [daww, match, background, colo, seem, stuck, t...  \n2  [hey, man, real, try, edit, war, guy, const, r...  \n3  [mak, real, suggest, improv, wond, sect, stat,...  \n4                    [sir, hero, chant, rememb, pag]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>text_clean</th>\n      <th>tokenized</th>\n      <th>stopwords_removed</th>\n      <th>snowball_stemmer</th>\n      <th>lancaster_stemmer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>explanation\\nwhy the edits made under my usern...</td>\n      <td>[explanation, why, the, edits, made, under, my...</td>\n      <td>[explanation, edits, made, username, hardcore,...</td>\n      <td>[explan, edit, made, usernam, hardcor, metalli...</td>\n      <td>[expl, edit, mad, usernam, hardc, metallic, fa...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>daww he matches this background colour i am se...</td>\n      <td>[daww, he, matches, this, background, colour, ...</td>\n      <td>[daww, matches, background, colour, seemingly,...</td>\n      <td>[daww, match, background, colour, seem, stuck,...</td>\n      <td>[daww, match, background, colo, seem, stuck, t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hey man i am really not trying to edit war it ...</td>\n      <td>[hey, man, i, am, really, not, trying, to, edi...</td>\n      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n      <td>[hey, man, realli, tri, edit, war, guy, consta...</td>\n      <td>[hey, man, real, try, edit, war, guy, const, r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>\\nmore\\ni cannot make any real suggestions on ...</td>\n      <td>[more, i, can, not, make, any, real, suggestio...</td>\n      <td>[make, real, suggestions, improvement, wondere...</td>\n      <td>[make, real, suggest, improv, wonder, section,...</td>\n      <td>[mak, real, suggest, improv, wond, sect, stat,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>you sir are my hero any chance you remember wh...</td>\n      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n      <td>[sir, hero, chance, remember, page]</td>\n      <td>[sir, hero, chanc, rememb, page]</td>\n      <td>[sir, hero, chant, rememb, pag]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}